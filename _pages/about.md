---
permalink: /
title: "Sean(HaoJin) Wang"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am a senior undergraduate at [Tongji University](https://en.tongji.edu.cn/) in Shanghai, China. I major in computer science and technology at School of Electronic and Information Engineering and will be graduating in July, 2025. I am now working as a Visiting Undergraduate Student at the [University of Waterloo](https://uwaterloo.ca/) in the David R. Cheriton School of Computer Science, Faculty of Mathematics.

I am actively searching for internships, master programs and PhD opportunities which can allow me to take my research into next level.

Links: [CV](../assets/Sean.pdf)


Research Overview
======
My primary research interest lies in learning for Natural Language Processing (NLP), particularly in understanding the science of generative language models and providing a universal framework for comprehending their high-level behaviors. I focus on representation learning and language modeling, aiming to contribute to the development of **safer** and **more transparent** AI systems.

Some of my recent research ideas are:

1. **About Embeddings:**  
Word embeddings are crucial components for understanding the high-level behavior of current language models, yet our knowledge about them remains incomplete. In the future, I aim to address the following questions:  

   - **Can embedding-level alignment be further improved** to ensure the generation of safe and non-toxic outputs, considering that word embeddings encode information beyond merely representing tokens?  
   - **Can we uncover an ultimate explanation for the origin and meaning of embeddings**, providing deeper insights into their significance?  
   - **Is it possible to develop continuous representations of languages** and design structures that enable training from such representations, paving the way for training-free steering in everyday applications?  

2. **About Model Safety**
I am passionate about model safety, particularly in the area of **inverse language modeling**. Large language models' logits reveal both critical internal mechanisms and key features of their inputs, making them a focal point for enhancing AI safety. I aim to develop an **autoregressive inverse language modeling approach** to better understand and safeguard these systems. By exploring methods for both attacking and defending models, I seek to address vulnerabilities and pave the way for more robust and trustworthy next-generation AI.

3. **About Efficient Training and Fine-Tunings**
In resource-scarce scenarios such as for niche languages, current generative models may not exhibit equal performance. Pretraining language models is computationally intensive and data-hungry, but developing techniques that enable models to learn general rules during pretraining could significantly reduce their dependence on extensive datasets. In the future, I aim to find efficient training methods and fine-tuning methods which **rely on less data** while **exhibiting better robustness**. 

Updates
======
**September 2024**: I am working as a research assistant at [Compling Lab](https://compling-wat.com/) with Prof [Freda Shi](https://home.ttic.edu/~freda/) at [University of Waterloo](https://uwaterloo.ca/)! My first research project here focuses on exploring potential limitations in language modeling by analyzing output logits and input embeddings. It's an incredibly exciting and enjoyable project!

**Jan 2024**: I am studying as an exchange student at [National University of Singapore](https://nus.edu.sg/), and I will be taking 3 courses here including [CS2108: Introduction to Media Computing](https://nusmods.com/courses/CS2108/introduction-to-media-computing), [CS3245: Information Retrieval](https://nusmods.com/courses/CS3245/information-retrieval) and [IS3107: Data Engineering](https://nusmods.com/courses/IS3107/data-engineering).

Publications
======
Expecting to come in 2025.